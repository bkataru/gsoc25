{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef6a013-e6d4-445c-abdf-ead55e1d3a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Mixed Precision Training Enabled: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Use standard TensorDataset now\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset, TensorDataset\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# --- Configuration ---\n",
    "PHOTON_FILE = 'photons.hdf5'\n",
    "ELECTRON_FILE = 'electrons.hdf5'\n",
    "MODEL_SAVE_PATH = 'electron_photon_resnet15_v4_ram.pth' # Suffix v4_ram\n",
    "OPTIMIZER_SAVE_PATH = MODEL_SAVE_PATH + \".opt\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "SEED = 42\n",
    "BATCH_SIZE = 512      # Can potentially increase further with RAM loading\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_EPOCHS = 30\n",
    "NUM_WORKERS = 0         # Workers mainly help with batch collation now\n",
    "TRAIN_SPLIT = 0.7\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "# --- Feature Flags ---\n",
    "USE_LOG_TRANSFORM = True # Apply log(1+E) during preprocessing\n",
    "USE_AUGMENTATION = True # Apply light augmentations during training\n",
    "USE_MIXED_PRECISION = torch.cuda.is_available()\n",
    "\n",
    "# --- Reproducibility ---\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "# Optional: torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Mixed Precision Training Enabled: {USE_MIXED_PRECISION}\")\n",
    "\n",
    "# --- File Checks ---\n",
    "if not os.path.exists(PHOTON_FILE): raise FileNotFoundError(f\"Photon file not found: {PHOTON_FILE}\")\n",
    "if not os.path.exists(ELECTRON_FILE): raise FileNotFoundError(f\"Electron file not found: {ELECTRON_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88577c3f-3bed-41dc-b3cc-d5459a6e47bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from HDF5 files to RAM...\n",
      "Loaded 249000 photons and 249000 electrons.\n",
      "Data loaded into RAM in 26.76s.\n",
      "Combined X shape: (498000, 32, 32, 2), dtype: float32\n",
      "Combined y shape: (498000, 1), dtype: float32\n",
      "Moving data to device: cuda...\n",
      "Data moved to cuda in 1.08s.\n",
      "GPU Memory Used (X): 3.80 GB\n",
      "GPU Memory Used (y): 0.00 GB\n",
      "Total GPU Memory Used: 3.80 GB\n"
     ]
    }
   ],
   "source": [
    "def load_data_to_device(photon_file, electron_file, device):\n",
    "    print(\"Loading data from HDF5 files to RAM...\")\n",
    "    start_time = time.time()\n",
    "    with h5py.File(photon_file, 'r') as f:\n",
    "        photon_X_np = f['X'][:] # Shape (N, H, W, C)\n",
    "        photon_y_np = f['y'][:] # Shape (N,)\n",
    "        num_photons = len(photon_y_np)\n",
    "    with h5py.File(electron_file, 'r') as f:\n",
    "        electron_X_np = f['X'][:]\n",
    "        electron_y_np = f['y'][:]\n",
    "        num_electrons = len(electron_y_np)\n",
    "\n",
    "    print(f\"Loaded {num_photons} photons and {num_electrons} electrons.\")\n",
    "\n",
    "    # Concatenate NumPy arrays\n",
    "    all_X_np = np.concatenate([photon_X_np, electron_X_np], axis=0).astype(np.float32)\n",
    "    all_y_np = np.concatenate([photon_y_np, electron_y_np], axis=0).astype(np.float32)\n",
    "\n",
    "    # Add channel dimension for labels (N, 1)\n",
    "    all_y_np = np.expand_dims(all_y_np, axis=-1)\n",
    "\n",
    "    ram_load_time = time.time() - start_time\n",
    "    print(f\"Data loaded into RAM in {ram_load_time:.2f}s.\")\n",
    "    print(f\"Combined X shape: {all_X_np.shape}, dtype: {all_X_np.dtype}\")\n",
    "    print(f\"Combined y shape: {all_y_np.shape}, dtype: {all_y_np.dtype}\")\n",
    "\n",
    "    print(f\"Moving data to device: {device}...\")\n",
    "    start_time = time.time()\n",
    "    # Convert to PyTorch tensors and move to device\n",
    "    # Keep X in (N, H, W, C) format for now, preprocessing will handle permute\n",
    "    all_X_tensor = torch.from_numpy(all_X_np).to(device)\n",
    "    all_y_tensor = torch.from_numpy(all_y_np).to(device)\n",
    "    gpu_load_time = time.time() - start_time\n",
    "    print(f\"Data moved to {device} in {gpu_load_time:.2f}s.\")\n",
    "\n",
    "    # Verify memory usage on GPU (optional)\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU Memory Used (X): {all_X_tensor.element_size() * all_X_tensor.nelement() / (1024**3):.2f} GB\")\n",
    "        print(f\"GPU Memory Used (y): {all_y_tensor.element_size() * all_y_tensor.nelement() / (1024**3):.2f} GB\")\n",
    "        print(f\"Total GPU Memory Used: {(all_X_tensor.element_size() * all_X_tensor.nelement() + all_y_tensor.element_size() * all_y_tensor.nelement()) / (1024**3):.2f} GB\")\n",
    "\n",
    "\n",
    "    return all_X_tensor, all_y_tensor, num_photons, num_electrons\n",
    "\n",
    "# Load data onto the target device\n",
    "ALL_X_DEVICE, ALL_Y_DEVICE, num_photons, num_electrons = load_data_to_device(PHOTON_FILE, ELECTRON_FILE, device)\n",
    "total_samples = num_photons + num_electrons\n",
    "\n",
    "# Define labels (already known but good to have)\n",
    "PHOTON_LABEL = 0.0\n",
    "ELECTRON_LABEL = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68422e0-4a9e-4095-a567-173c9c7a8981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing full dataset tensor on GPU...\n",
      "Applying log1p transform to energy channel...\n",
      "Shape before normalization (NCHW): torch.Size([498000, 2, 32, 32])\n",
      "Calculating mean and std dev on GPU (post-log transform)...\n",
      "Calculated Mean (GPU): [ 0.00102406 -0.00026181]\n",
      "Calculated Std Dev (GPU): [0.01807128 0.06738362]\n",
      "Applying normalization...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.80 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_tensor_processed_nchw, mean_gpu, std_gpu, x_tensor_prenorm_nchw\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Preprocess the data ONCE\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m ALL_X_PROC_DEVICE, GLOBAL_MEAN_GPU, GLOBAL_STD_GPU, ALL_X_PRENORM_DEVICE \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_full_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mALL_X_DEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSE_LOG_TRANSFORM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# We no longer need the original raw tensor on the GPU if memory is a concern\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# del ALL_X_DEVICE\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mpreprocess_full_tensor\u001b[0;34m(x_tensor_nhwc, use_log)\u001b[0m\n\u001b[1;32m     42\u001b[0m std_reshaped \u001b[38;5;241m=\u001b[39m std_gpu\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Apply normalization to the log-transformed tensor\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m x_tensor_normalized_nhwc \u001b[38;5;241m=\u001b[39m (\u001b[43mx_working\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean_reshaped\u001b[49m) \u001b[38;5;241m/\u001b[39m std_reshaped\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 4. Permute the *normalized* tensor to (N, C, H, W) for the model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPermuting normalized tensor to NCHW format...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.80 GiB. GPU "
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def preprocess_full_tensor(x_tensor_nhwc, use_log): # Input is NHWC\n",
    "    \"\"\"\n",
    "    Applies log transform, calculates stats, normalizes, and permutes.\n",
    "    Returns:\n",
    "        x_tensor_processed (NCHW, normalized),\n",
    "        mean_gpu,\n",
    "        std_gpu,\n",
    "        x_tensor_prenorm_nchw (NCHW, log-transformed but not normalized)\n",
    "    \"\"\"\n",
    "    print(\"\\nPreprocessing full dataset tensor on GPU...\")\n",
    "    start_time = time.time()\n",
    "    # Input x_tensor shape: (N, H, W, C)\n",
    "\n",
    "    # Create a working copy to avoid modifying the original ALL_X_DEVICE if needed later\n",
    "    x_working = x_tensor_nhwc.clone()\n",
    "\n",
    "    # 1. Log Transform (Energy channel: index 0) - In-place on the clone\n",
    "    if use_log:\n",
    "        print(\"Applying log1p transform to energy channel...\")\n",
    "        x_working[:, :, :, 0] = torch.log1p(torch.relu(x_working[:, :, :, 0]))\n",
    "\n",
    "    # Store the pre-normalization tensor (after log transform)\n",
    "    # Permute it now to NCHW for consistency in return values\n",
    "    x_tensor_prenorm_nchw = x_working.permute(0, 3, 1, 2).contiguous()\n",
    "    print(f\"Shape before normalization (NCHW): {x_tensor_prenorm_nchw.shape}\")\n",
    "\n",
    "\n",
    "    # 2. Calculate Mean and Std Dev directly on GPU tensor (using the log-transformed data)\n",
    "    print(\"Calculating mean and std dev on GPU (post-log transform)...\")\n",
    "    # Calculate over N, H, W dimensions of the NHWC tensor, keep C dimension\n",
    "    mean_gpu = torch.mean(x_working, dim=(0, 1, 2), keepdim=False)\n",
    "    std_gpu = torch.std(x_working, dim=(0, 1, 2), keepdim=False)\n",
    "    std_gpu = torch.clamp(std_gpu, min=1e-6)\n",
    "    print(f\"Calculated Mean (GPU): {mean_gpu.cpu().numpy()}\")\n",
    "    print(f\"Calculated Std Dev (GPU): {std_gpu.cpu().numpy()}\")\n",
    "\n",
    "    # 3. Normalize the tensor (using the log-transformed data)\n",
    "    print(\"Applying normalization...\")\n",
    "    # Reshape mean/std for broadcasting: (1, 1, 1, C) for NHWC tensor\n",
    "    mean_reshaped = mean_gpu.view(1, 1, 1, -1)\n",
    "    std_reshaped = std_gpu.view(1, 1, 1, -1)\n",
    "    # Apply normalization to the log-transformed tensor\n",
    "    x_tensor_normalized_nhwc = (x_working - mean_reshaped) / std_reshaped\n",
    "\n",
    "    # 4. Permute the *normalized* tensor to (N, C, H, W) for the model\n",
    "    print(\"Permuting normalized tensor to NCHW format...\")\n",
    "    x_tensor_processed_nchw = x_tensor_normalized_nhwc.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    print(f\"Preprocessing completed in {time.time() - start_time:.2f}s.\")\n",
    "    print(f\"Final processed X tensor shape (normalized NCHW): {x_tensor_processed_nchw.shape}\")\n",
    "\n",
    "    # Clean up intermediate tensor if needed\n",
    "    del x_working, x_tensor_normalized_nhwc\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return x_tensor_processed_nchw, mean_gpu, std_gpu, x_tensor_prenorm_nchw\n",
    "\n",
    "# Preprocess the data ONCE\n",
    "ALL_X_PROC_DEVICE, GLOBAL_MEAN_GPU, GLOBAL_STD_GPU, ALL_X_PRENORM_DEVICE = preprocess_full_tensor(ALL_X_DEVICE, USE_LOG_TRANSFORM)\n",
    "\n",
    "# We no longer need the original raw tensor on the GPU if memory is a concern\n",
    "# del ALL_X_DEVICE\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214551ad-064d-4e41-a96f-9d9216d703c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_from_tensor(x_tensor_nchw, y_tensor, index, particle_type_str, title_suffix=\"\"):\n",
    "    \"\"\"Plots Energy and Time channels for a single sample from a tensor.\"\"\"\n",
    "    # Input x_tensor_nchw shape: (N, C, H, W)\n",
    "    img_chw = x_tensor_nchw[index].cpu().numpy() # Move to CPU for plotting\n",
    "    label_val = y_tensor[index].item()\n",
    "\n",
    "    energy, time = img_chw[0, :, :], img_chw[1, :, :] # C=0 is Energy, C=1 is Time\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    fig.suptitle(f\"{particle_type_str} Sample (Index: {index}, Label: {label_val}) {title_suffix}\", fontsize=14)\n",
    "\n",
    "    im_energy = axes[0].imshow(energy, cmap='viridis', aspect='auto')\n",
    "    axes[0].set_title(f'Energy Channel {title_suffix}')\n",
    "    fig.colorbar(im_energy, ax=axes[0])\n",
    "\n",
    "    im_time = axes[1].imshow(time, cmap='plasma', aspect='auto')\n",
    "    axes[1].set_title(f'Time Channel {title_suffix}')\n",
    "    fig.colorbar(im_time, ax=axes[1])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def plot_histograms_comparison(x_prenorm_nchw, x_postnorm_nchw, y_tensor, num_samples_hist=50000, title_prefix=\"\"):\n",
    "    \"\"\"Plots histograms comparing pre- and post-normalization distributions.\"\"\"\n",
    "    print(f\"\\nPlotting {title_prefix} histograms using {num_samples_hist} random samples...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Ensure tensors are on the correct device (should already be, but safe check)\n",
    "    x_prenorm_nchw = x_prenorm_nchw.to(device)\n",
    "    x_postnorm_nchw = x_postnorm_nchw.to(device)\n",
    "    y_tensor = y_tensor.to(device)\n",
    "\n",
    "\n",
    "    plot_indices = np.random.choice(x_prenorm_nchw.shape[0], num_samples_hist, replace=False)\n",
    "\n",
    "    # Sample data directly from GPU tensors\n",
    "    sampled_x_prenorm = x_prenorm_nchw[plot_indices]\n",
    "    sampled_x_postnorm = x_postnorm_nchw[plot_indices]\n",
    "    sampled_y = y_tensor[plot_indices]\n",
    "\n",
    "    # Separate photons and electrons\n",
    "    photon_mask = (sampled_y == PHOTON_LABEL).squeeze()\n",
    "    electron_mask = (sampled_y == ELECTRON_LABEL).squeeze()\n",
    "\n",
    "    # --- Extract Pre-Normalization Data ---\n",
    "    photon_energies_pre = sampled_x_prenorm[photon_mask, 0, :, :].flatten().cpu().numpy()\n",
    "    photon_times_pre = sampled_x_prenorm[photon_mask, 1, :, :].flatten().cpu().numpy()\n",
    "    electron_energies_pre = sampled_x_prenorm[electron_mask, 0, :, :].flatten().cpu().numpy()\n",
    "    electron_times_pre = sampled_x_prenorm[electron_mask, 1, :, :].flatten().cpu().numpy()\n",
    "\n",
    "    # --- Extract Post-Normalization Data ---\n",
    "    photon_energies_post = sampled_x_postnorm[photon_mask, 0, :, :].flatten().cpu().numpy()\n",
    "    photon_times_post = sampled_x_postnorm[photon_mask, 1, :, :].flatten().cpu().numpy()\n",
    "    electron_energies_post = sampled_x_postnorm[electron_mask, 0, :, :].flatten().cpu().numpy()\n",
    "    electron_times_post = sampled_x_postnorm[electron_mask, 1, :, :].flatten().cpu().numpy()\n",
    "\n",
    "    del sampled_x_prenorm, sampled_x_postnorm, sampled_y\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Filter non-finite/zero values for plotting\n",
    "    def filter_data(arr):\n",
    "        # Keep finite values, allow zeros for post-norm data\n",
    "        return arr[np.isfinite(arr)]\n",
    "\n",
    "    def filter_data_pre(arr):\n",
    "         # Keep finite values, filter near-zero for pre-norm log scale\n",
    "        return arr[np.isfinite(arr) & (np.abs(arr) > 1e-6)]\n",
    "\n",
    "\n",
    "    photon_energies_pre_nz = filter_data_pre(photon_energies_pre)\n",
    "    electron_energies_pre_nz = filter_data_pre(electron_energies_pre)\n",
    "    photon_times_pre_nz = filter_data_pre(photon_times_pre)\n",
    "    electron_times_pre_nz = filter_data_pre(electron_times_pre)\n",
    "\n",
    "    photon_energies_post_f = filter_data(photon_energies_post)\n",
    "    electron_energies_post_f = filter_data(electron_energies_post)\n",
    "    photon_times_post_f = filter_data(photon_times_post)\n",
    "    electron_times_post_f = filter_data(electron_times_post)\n",
    "\n",
    "    print(f\"Histogram data preparation took {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(24, 10)) # 2 rows, 4 columns\n",
    "    fig.suptitle(f\"{title_prefix} Channel Value Distributions Comparison\", fontsize=16)\n",
    "\n",
    "    # Column Titles\n",
    "    axes[0, 0].set_title('Energy Pre-Norm (Linear)')\n",
    "    axes[0, 1].set_title('Energy Pre-Norm (Log Scale)')\n",
    "    axes[0, 2].set_title('Energy Post-Norm (Linear)')\n",
    "    axes[0, 3].set_title('Energy Post-Norm (Symlog Scale)') # Changed to symlog\n",
    "    axes[1, 0].set_title('Time Pre-Norm (Linear)')\n",
    "    axes[1, 1].set_title('Time Pre-Norm (Log Scale)')\n",
    "    axes[1, 2].set_title('Time Post-Norm (Linear)')\n",
    "    axes[1, 3].set_title('Time Post-Norm (Symlog Scale)') # Changed to symlog\n",
    "\n",
    "    # --- Pre-Normalization Plots ---\n",
    "    # Energy Linear Pre\n",
    "    axes[0, 0].hist(photon_energies_pre_nz, bins=100, alpha=0.7, label='Photons', density=True)\n",
    "    axes[0, 0].hist(electron_energies_pre_nz, bins=100, alpha=0.7, label='Electrons', density=True)\n",
    "    axes[0, 0].set_xlabel('Energy (Log-Transformed)')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Energy Log Pre\n",
    "    min_e_pre = min(photon_energies_pre_nz.min(), electron_energies_pre_nz.min()) if len(photon_energies_pre_nz)>0 and len(electron_energies_pre_nz)>0 else 1e-5\n",
    "    max_e_pre = max(photon_energies_pre_nz.max(), electron_energies_pre_nz.max()) if len(photon_energies_pre_nz)>0 and len(electron_energies_pre_nz)>0 else 1.0\n",
    "    log_bins_e_pre = np.logspace(np.log10(max(1e-5, min_e_pre)), np.log10(max_e_pre), 100)\n",
    "    axes[0, 1].hist(photon_energies_pre_nz, bins=log_bins_e_pre, alpha=0.7, label='Photons', density=True)\n",
    "    axes[0, 1].hist(electron_energies_pre_nz, bins=log_bins_e_pre, alpha=0.7, label='Electrons', density=True)\n",
    "    axes[0, 1].set_xlabel('Energy (Log-Transformed)')\n",
    "    axes[0, 1].set_xscale('log')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Time Linear Pre\n",
    "    axes[1, 0].hist(photon_times_pre_nz, bins=100, alpha=0.7, label='Photons', density=True)\n",
    "    axes[1, 0].hist(electron_times_pre_nz, bins=100, alpha=0.7, label='Electrons', density=True)\n",
    "    axes[1, 0].set_xlabel('Time')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Time Log Pre\n",
    "    min_t_pre = min(photon_times_pre_nz.min(), electron_times_pre_nz.min()) if len(photon_times_pre_nz)>0 and len(electron_times_pre_nz)>0 else 1e-5\n",
    "    max_t_pre = max(photon_times_pre_nz.max(), electron_times_pre_nz.max()) if len(photon_times_pre_nz)>0 and len(electron_times_pre_nz)>0 else 1.0\n",
    "    log_bins_t_pre = np.logspace(np.log10(max(1e-5, min_t_pre)), np.log10(max_t_pre), 100)\n",
    "    axes[1, 1].hist(photon_times_pre_nz, bins=log_bins_t_pre, alpha=0.7, label='Photons', density=True)\n",
    "    axes[1, 1].hist(electron_times_pre_nz, bins=log_bins_t_pre, alpha=0.7, label='Electrons', density=True)\n",
    "    axes[1, 1].set_xlabel('Time')\n",
    "    axes[1, 1].set_xscale('log')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # --- Post-Normalization Plots ---\n",
    "    # Energy Linear Post\n",
    "    axes[0, 2].hist(photon_energies_post_f, bins=100, range=(-5, 50), alpha=0.7, label='Photons', density=True) # Adjust range if needed\n",
    "    axes[0, 2].hist(electron_energies_post_f, bins=100, range=(-5, 50), alpha=0.7, label='Electrons', density=True)\n",
    "    axes[0, 2].set_xlabel('Processed Energy (Normalized)')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Energy Symlog Post (Better for data centered around zero)\n",
    "    axes[0, 3].hist(photon_energies_post_f, bins=100, alpha=0.7, label='Photons', density=True)\n",
    "    axes[0, 3].hist(electron_energies_post_f, bins=100, alpha=0.7, label='Electrons', density=True)\n",
    "    axes[0, 3].set_xlabel('Processed Energy (Normalized)')\n",
    "    axes[0, 3].set_xscale('symlog', linthresh=0.1) # Use symlog to see values near zero better\n",
    "    axes[0, 3].legend()\n",
    "    axes[0, 3].grid(True, alpha=0.3)\n",
    "\n",
    "    # Time Linear Post\n",
    "    axes[1, 2].hist(photon_times_post_f, bins=100, range=(-10, 10), alpha=0.7, label='Photons', density=True) # Adjust range\n",
    "    axes[1, 2].hist(electron_times_post_f, bins=100, range=(-10, 10), alpha=0.7, label='Electrons', density=True)\n",
    "    axes[1, 2].set_xlabel('Processed Time (Normalized)')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Time Symlog Post\n",
    "    axes[1, 3].hist(photon_times_post_f, bins=100, alpha=0.7, label='Photons', density=True)\n",
    "    axes[1, 3].hist(electron_times_post_f, bins=100, alpha=0.7, label='Electrons', density=True)\n",
    "    axes[1, 3].set_xlabel('Processed Time (Normalized)')\n",
    "    axes[1, 3].set_xscale('symlog', linthresh=0.1)\n",
    "    axes[1, 3].legend()\n",
    "    axes[1, 3].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Perform Visualizations ---\n",
    "print(\"Plotting sample images from processed tensor...\")\n",
    "# Plot using original global indices\n",
    "plot_sample_from_tensor(ALL_X_PROC_DEVICE, ALL_Y_DEVICE, 0, \"Photon\", title_suffix=\"(Processed)\")\n",
    "plot_sample_from_tensor(ALL_X_PROC_DEVICE, ALL_Y_DEVICE, num_photons, \"Electron\", title_suffix=\"(Processed)\")\n",
    "\n",
    "# Plot the comparison histograms\n",
    "# Assumes ALL_X_PRENORM_DEVICE was returned by the modified preprocess_full_tensor\n",
    "plot_histograms_comparison(\n",
    "    ALL_X_PRENORM_DEVICE,\n",
    "    ALL_X_PROC_DEVICE,\n",
    "    ALL_Y_DEVICE,\n",
    "    title_prefix=\"Pre- vs. Post-Normalization\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b854704-85d4-41c2-8f22-cd9b79ec7430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "ResNet15(\n",
      "  (conv1): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Total Parameters: 2,848,769\n",
      "Trainable Parameters: 2,848,769\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False); self.bn1 = nn.BatchNorm2d(planes); self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False); self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample; self.stride = stride\n",
    "    def forward(self, x):\n",
    "        identity = x; out = self.conv1(x); out = self.bn1(out); out = self.relu(out); out = self.conv2(out); out = self.bn2(out)\n",
    "        if self.downsample is not None: identity = self.downsample(x)\n",
    "        out += identity; out = self.relu(out); return out\n",
    "\n",
    "class ResNet15(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, layers=[3, 2, 2], num_classes=1, in_channels=2, dropout_prob=0.3):\n",
    "        super(ResNet15, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        # Input: (B, C, H, W) = (B, 2, 32, 32)\n",
    "        self.conv1 = nn.Conv2d(in_channels, self.in_planes, kernel_size=3, stride=1, padding=1, bias=False) # Output: (B, 64, 32, 32)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # No MaxPool\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1) # Output: (B, 64, 32, 32)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) # Output: (B, 128, 16, 16)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2) # Output: (B, 256, 8, 8)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # Output: (B, 256, 1, 1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(256 * block.expansion, num_classes) # Output: (B, 1)\n",
    "\n",
    "        # Weight Initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1); nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = []; layers.append(block(self.in_planes, planes, stride, downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, num_blocks): layers.append(block(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expects input x: (B, C, H, W)\n",
    "        x = self.conv1(x); x = self.bn1(x); x = self.relu(x)\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # Return raw logits\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet15().to(device)\n",
    "print(f\"Model Architecture:\\n{model}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\"); print(f\"Trainable Parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389347fa-04b0-4d0b-9829-0e6b4d0581d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Split Indices:\n",
      "  Training: 348600 indices\n",
      "  Validation: 49800 indices\n",
      "  Test: 99600 indices\n",
      "\n",
      "DataLoaders created: Batch Size=512, Num Workers=0, Persistent=False\n",
      "\n",
      "Loading model weights from electron_photon_resnet15_v4_ram.pth...\n",
      "Model weights loaded.\n",
      "Loading optimizer/scheduler state from electron_photon_resnet15_v4_ram.pth.opt...\n",
      "Optimizer and scheduler state loaded. Resuming from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# --- Data Splitting (Indices for pre-loaded tensors) ---\n",
    "indices = np.arange(total_samples); np.random.shuffle(indices)\n",
    "test_split_idx = int(np.floor(TEST_SPLIT * total_samples)); val_split_idx = int(np.floor((VAL_SPLIT + TEST_SPLIT) * total_samples))\n",
    "test_indices = indices[:test_split_idx]; val_indices = indices[test_split_idx:val_split_idx]; train_indices = indices[val_split_idx:]\n",
    "print(f\"\\nDataset Split Indices:\"); print(f\"  Training: {len(train_indices)} indices\"); print(f\"  Validation: {len(val_indices)} indices\"); print(f\"  Test: {len(test_indices)} indices\")\n",
    "\n",
    "# --- Create TensorDatasets for each split ---\n",
    "# Index the pre-processed tensors using the split indices\n",
    "train_dataset = TensorDataset(ALL_X_PROC_DEVICE[train_indices], ALL_Y_DEVICE[train_indices])\n",
    "val_dataset = TensorDataset(ALL_X_PROC_DEVICE[val_indices], ALL_Y_DEVICE[val_indices])\n",
    "test_dataset = TensorDataset(ALL_X_PROC_DEVICE[test_indices], ALL_Y_DEVICE[test_indices])\n",
    "\n",
    "# --- Create Standard DataLoaders ---\n",
    "persist_workers = NUM_WORKERS > 0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, # Use standard shuffle\n",
    "                          num_workers=NUM_WORKERS,\n",
    "                          persistent_workers=persist_workers, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        persistent_workers=persist_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS,\n",
    "                         persistent_workers=persist_workers)\n",
    "\n",
    "print(f\"\\nDataLoaders created: Batch Size={BATCH_SIZE}, Num Workers={NUM_WORKERS}, Persistent={persist_workers}\")\n",
    "\n",
    "# --- Loss, Optimizer, Scheduler, Scaler ---\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)\n",
    "scaler = GradScaler(enabled=USE_MIXED_PRECISION)\n",
    "\n",
    "# --- Load Checkpoint Logic ---\n",
    "# (Same as your analysis3.md cell 7)\n",
    "start_epoch = 0; # ... (rest of checkpoint loading logic)\n",
    "if os.path.exists(MODEL_SAVE_PATH): # ...\n",
    "    print(f\"\\nLoading model weights from {MODEL_SAVE_PATH}...\"); # ...\n",
    "    try: # ...\n",
    "        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device)); print(\"Model weights loaded.\")\n",
    "        if os.path.exists(OPTIMIZER_SAVE_PATH): # ...\n",
    "             print(f\"Loading optimizer/scheduler state from {OPTIMIZER_SAVE_PATH}...\"); # ...\n",
    "             checkpoint = torch.load(OPTIMIZER_SAVE_PATH, map_location='cpu'); optimizer.load_state_dict(checkpoint['optimizer']); scheduler.load_state_dict(checkpoint['scheduler']); start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "             print(f\"Optimizer and scheduler state loaded. Resuming from epoch {start_epoch}\")\n",
    "        else: print(\"Optimizer state file not found, initializing optimizer from scratch.\")\n",
    "    except Exception as e: print(f\"WARNING: Could not load checkpoint: {e}. Training from scratch.\"); start_epoch = 0\n",
    "else: print(\"\\nNo checkpoint found. Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac555099-6dac-4704-8079-41b23b2022e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming with best_val_loss = 0.5433\n",
      "\n",
      "--- Starting Training (RAM Loaded Data) ---\n",
      "Running for 27 epochs (from epoch 3 to 29)\n",
      "\n",
      "Epoch 4/30, Current LR: 0.000500\n",
      "  Batch    0/680, Loss: 0.5278, Acc: 0.7520, Batch Time: 597.0ms\n",
      "  Batch  100/680, Loss: 0.5718, Acc: 0.7188, Batch Time: 148.2ms\n",
      "  Batch  200/680, Loss: 0.5072, Acc: 0.7402, Batch Time: 147.4ms\n",
      "  Batch  300/680, Loss: 0.5145, Acc: 0.7578, Batch Time: 147.4ms\n",
      "  Batch  400/680, Loss: 0.5327, Acc: 0.7344, Batch Time: 148.3ms\n",
      "  Batch  500/680, Loss: 0.5226, Acc: 0.7383, Batch Time: 147.8ms\n",
      "  Batch  600/680, Loss: 0.5795, Acc: 0.7051, Batch Time: 148.5ms\n",
      "  Epoch Training Time: 104.22s\n",
      "  Validation Time: 5.01s\n",
      "------------------------------------------------------------\n",
      "Epoch 4 Summary:\n",
      "  Train Loss: 0.5501, Train Acc: 0.7270\n",
      "  Val Loss:   0.5500, Val Acc:   0.7246\n",
      "  Duration: 109.23s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 5/30, Current LR: 0.000500\n",
      "  Batch    0/680, Loss: 0.5410, Acc: 0.7461, Batch Time: 145.0ms\n",
      "  Batch  100/680, Loss: 0.5464, Acc: 0.7363, Batch Time: 147.1ms\n",
      "  Batch  200/680, Loss: 0.5510, Acc: 0.7148, Batch Time: 150.0ms\n",
      "  Batch  300/680, Loss: 0.6056, Acc: 0.6855, Batch Time: 149.4ms\n",
      "  Batch  400/680, Loss: 0.5467, Acc: 0.7402, Batch Time: 148.6ms\n",
      "  Batch  500/680, Loss: 0.5679, Acc: 0.7324, Batch Time: 147.6ms\n",
      "  Batch  600/680, Loss: 0.5782, Acc: 0.6895, Batch Time: 148.8ms\n",
      "  Epoch Training Time: 104.28s\n",
      "  Validation Time: 5.00s\n",
      "------------------------------------------------------------\n",
      "Epoch 5 Summary:\n",
      "  Train Loss: 0.5481, Train Acc: 0.7283\n",
      "  Val Loss:   0.5486, Val Acc:   0.7295\n",
      "  Duration: 109.28s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 6/30, Current LR: 0.000500\n",
      "  Batch    0/680, Loss: 0.5494, Acc: 0.7402, Batch Time: 147.7ms\n",
      "  Batch  100/680, Loss: 0.5807, Acc: 0.7305, Batch Time: 148.7ms\n",
      "  Batch  200/680, Loss: 0.5328, Acc: 0.7402, Batch Time: 148.1ms\n",
      "  Batch  300/680, Loss: 0.5658, Acc: 0.7266, Batch Time: 147.5ms\n",
      "  Batch  400/680, Loss: 0.5900, Acc: 0.6816, Batch Time: 148.2ms\n",
      "  Batch  500/680, Loss: 0.5333, Acc: 0.7344, Batch Time: 147.5ms\n",
      "  Batch  600/680, Loss: 0.5531, Acc: 0.7324, Batch Time: 149.9ms\n",
      "  Epoch Training Time: 104.40s\n",
      "  Validation Time: 4.98s\n",
      "------------------------------------------------------------\n",
      "Epoch 6 Summary:\n",
      "  Train Loss: 0.5468, Train Acc: 0.7294\n",
      "  Val Loss:   0.5485, Val Acc:   0.7256\n",
      "  Duration: 109.39s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 7/30, Current LR: 0.000500\n",
      "  Batch    0/680, Loss: 0.5677, Acc: 0.7168, Batch Time: 147.1ms\n",
      "  Batch  100/680, Loss: 0.5203, Acc: 0.7637, Batch Time: 149.5ms\n",
      "  Batch  200/680, Loss: 0.5038, Acc: 0.7500, Batch Time: 149.2ms\n",
      "  Batch  300/680, Loss: 0.5640, Acc: 0.7148, Batch Time: 148.7ms\n",
      "  Batch  400/680, Loss: 0.5569, Acc: 0.7363, Batch Time: 150.6ms\n",
      "  Batch  500/680, Loss: 0.6061, Acc: 0.6816, Batch Time: 150.3ms\n",
      "  Batch  600/680, Loss: 0.5555, Acc: 0.7246, Batch Time: 149.7ms\n",
      "  Epoch Training Time: 104.49s\n",
      "  Validation Time: 4.98s\n",
      "------------------------------------------------------------\n",
      "Epoch 7 Summary:\n",
      "  Train Loss: 0.5452, Train Acc: 0.7301\n",
      "  Val Loss:   0.5395, Val Acc:   0.7333\n",
      "  Duration: 109.47s\n",
      "------------------------------------------------------------\n",
      "Validation loss improved (0.5433 -> 0.5395). Saving model...\n",
      "\n",
      "Epoch 8/30, Current LR: 0.000500\n",
      "  Batch    0/680, Loss: 0.5760, Acc: 0.6934, Batch Time: 146.9ms\n",
      "  Batch  100/680, Loss: 0.5487, Acc: 0.7344, Batch Time: 149.5ms\n",
      "  Batch  200/680, Loss: 0.5787, Acc: 0.7148, Batch Time: 148.6ms\n",
      "  Batch  300/680, Loss: 0.5419, Acc: 0.7227, Batch Time: 149.5ms\n",
      "  Batch  400/680, Loss: 0.5468, Acc: 0.7266, Batch Time: 148.4ms\n",
      "  Batch  500/680, Loss: 0.5539, Acc: 0.7227, Batch Time: 148.4ms\n",
      "  Batch  600/680, Loss: 0.5571, Acc: 0.7109, Batch Time: 150.0ms\n",
      "  Epoch Training Time: 104.63s\n",
      "  Validation Time: 5.10s\n",
      "------------------------------------------------------------\n",
      "Epoch 8 Summary:\n",
      "  Train Loss: 0.5438, Train Acc: 0.7311\n",
      "  Val Loss:   0.5402, Val Acc:   0.7298\n",
      "  Duration: 109.73s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 9/30, Current LR: 0.000500\n",
      "  Batch    0/680, Loss: 0.5554, Acc: 0.7129, Batch Time: 147.0ms\n",
      "  Batch  100/680, Loss: 0.5375, Acc: 0.7207, Batch Time: 148.4ms\n",
      "  Batch  200/680, Loss: 0.5561, Acc: 0.7148, Batch Time: 149.2ms\n",
      "  Batch  300/680, Loss: 0.5157, Acc: 0.7520, Batch Time: 149.4ms\n",
      "  Batch  400/680, Loss: 0.5378, Acc: 0.7383, Batch Time: 148.0ms\n",
      "  Batch  500/680, Loss: 0.5220, Acc: 0.7480, Batch Time: 148.5ms\n",
      "  Batch  600/680, Loss: 0.5874, Acc: 0.6777, Batch Time: 147.7ms\n",
      "  Epoch Training Time: 104.38s\n",
      "  Validation Time: 4.97s\n",
      "------------------------------------------------------------\n",
      "Epoch 9 Summary:\n",
      "  Train Loss: 0.5421, Train Acc: 0.7322\n",
      "  Val Loss:   0.5423, Val Acc:   0.7281\n",
      "  Duration: 109.35s\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 10/30, Current LR: 0.000500\n",
      "  Batch    0/680, Loss: 0.5135, Acc: 0.7539, Batch Time: 146.6ms\n",
      "  Batch  100/680, Loss: 0.5110, Acc: 0.7578, Batch Time: 148.4ms\n",
      "  Batch  200/680, Loss: 0.5475, Acc: 0.7344, Batch Time: 150.4ms\n",
      "  Batch  300/680, Loss: 0.5275, Acc: 0.7578, Batch Time: 149.6ms\n",
      "  Batch  400/680, Loss: 0.5191, Acc: 0.7500, Batch Time: 149.1ms\n",
      "  Batch  500/680, Loss: 0.5377, Acc: 0.7246, Batch Time: 150.0ms\n",
      "  Batch  600/680, Loss: 0.5223, Acc: 0.7500, Batch Time: 150.0ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime(); \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Current LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSE_MIXED_PRECISION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSE_AUGMENTATION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss); history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, criterion, optimizer, scaler, device, use_amp, use_augment)\u001b[0m\n\u001b[1;32m     36\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch_x) \u001b[38;5;66;03m# Logits\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m---> 38\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(); \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m; scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     40\u001b[0m predicted \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39msigmoid(outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(); correct \u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m batch_y)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     41\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m correct; batch_size \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m); total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size; total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch_size\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/amp/grad_scaler.py:453\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    451\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 453\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/amp/grad_scaler.py:350\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/amp/grad_scaler.py:350\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- GPU Augmentation Function ---\n",
    "@torch.no_grad()\n",
    "def augment_batch_gpu(batch_x, use_augment):\n",
    "    \"\"\"Applies augmentations to a batch on the GPU.\"\"\"\n",
    "    if not use_augment:\n",
    "        return batch_x\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    if random.random() < 0.5:\n",
    "         noise = torch.randn_like(batch_x) * 0.1\n",
    "         batch_x = batch_x + noise # Not in-place\n",
    "\n",
    "    # Add other GPU augmentations here (e.g., kornia for shifts/rotations)\n",
    "    # Example using torch.roll for tiny shifts:\n",
    "    # if random.random() < 0.2:\n",
    "    #    shift_h = random.randint(-1, 1)\n",
    "    #    shift_w = random.randint(-1, 1)\n",
    "    #    batch_x = torch.roll(batch_x, shifts=(shift_h, shift_w), dims=(2, 3))\n",
    "\n",
    "    return batch_x\n",
    "\n",
    "# --- Training and Validation Functions ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, device, use_amp, use_augment):\n",
    "    model.train(); total_loss = 0.0; total_correct = 0; total_samples = 0; loop_start_time = time.time()\n",
    "\n",
    "    for i, (batch_x, batch_y) in enumerate(loader): # Data is already on device from TensorDataset\n",
    "        batch_start_time = time.time()\n",
    "        # Data batch_x, batch_y are already on the correct device\n",
    "\n",
    "        # --- Apply Augmentation on GPU ---\n",
    "        batch_x = augment_batch_gpu(batch_x, use_augment)\n",
    "        # --------------------------------\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=device.type, enabled=use_amp):\n",
    "            outputs = model(batch_x) # Logits\n",
    "            loss = criterion(outputs, batch_y)\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float(); correct = (predicted == batch_y).float().sum().item()\n",
    "        total_correct += correct; batch_size = batch_y.size(0); total_samples += batch_size; total_loss += loss.item() * batch_size\n",
    "        batch_end_time = time.time()\n",
    "        if i % 100 == 0: print(f\"  Batch {i:>4}/{len(loader)}, Loss: {loss.item():.4f}, Acc: {correct/batch_size:.4f}, Batch Time: {(batch_end_time - batch_start_time)*1000:.1f}ms\")\n",
    "\n",
    "    avg_loss = total_loss / total_samples; avg_acc = total_correct / total_samples; print(f\"  Epoch Training Time: {time.time() - loop_start_time:.2f}s\")\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device, use_amp):\n",
    "    model.eval(); total_loss = 0.0; total_correct = 0; total_samples = 0; val_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader: # Data is already on device\n",
    "            # No augmentation during validation\n",
    "            with autocast(device_type=device.type, enabled=use_amp):\n",
    "                outputs = model(batch_x); loss = criterion(outputs, batch_y)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_correct += (predicted == batch_y).float().sum().item()\n",
    "            total_samples += batch_y.size(0); total_loss += loss.item() * batch_y.size(0)\n",
    "    avg_loss = total_loss / total_samples; avg_acc = total_correct / total_samples; print(f\"  Validation Time: {time.time() - val_start_time:.2f}s\")\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "history={'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}; best_val_loss = float('inf')\n",
    "if start_epoch > 0 and 'best' in scheduler.state_dict(): best_val_loss = scheduler.state_dict()['best'] if scheduler.state_dict()['best'] is not None else float('inf'); print(f\"Resuming with best_val_loss = {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n--- Starting Training (RAM Loaded Data) ---\"); epochs_to_run = NUM_EPOCHS - start_epoch\n",
    "if epochs_to_run <= 0: print(\"Training already completed.\")\n",
    "else: print(f\"Running for {epochs_to_run} epochs (from epoch {start_epoch} to {NUM_EPOCHS-1})\")\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    epoch_start_time = time.time(); print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}, Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    # Training\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, USE_MIXED_PRECISION, USE_AUGMENTATION)\n",
    "    history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "    # Validation\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device, USE_MIXED_PRECISION)\n",
    "    history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "    epoch_duration = time.time() - epoch_start_time; print(\"-\" * 60); print(f\"Epoch {epoch+1} Summary:\"); print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\"); print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"); print(f\"  Duration: {epoch_duration:.2f}s\"); print(\"-\" * 60)\n",
    "    scheduler.step(val_loss) # Scheduler Step\n",
    "    if val_loss < best_val_loss: # Save best model\n",
    "        print(f\"Validation loss improved ({best_val_loss:.4f} -> {val_loss:.4f}). Saving model...\"); best_val_loss = val_loss; torch.save(model.state_dict(), MODEL_SAVE_PATH); checkpoint = {'optimizer': optimizer.state_dict(), 'scheduler': scheduler.state_dict(), 'epoch': epoch}; torch.save(checkpoint, OPTIMIZER_SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58d3f4-d29d-43e1-8d32-7a1674c837f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
